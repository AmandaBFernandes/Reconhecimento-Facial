{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"faces.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>-0.044982</td>\n",
       "      <td>0.141337</td>\n",
       "      <td>0.669841</td>\n",
       "      <td>-1.055132</td>\n",
       "      <td>-0.554959</td>\n",
       "      <td>-0.094389</td>\n",
       "      <td>0.808895</td>\n",
       "      <td>0.614182</td>\n",
       "      <td>0.682296</td>\n",
       "      <td>0.018864</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020778</td>\n",
       "      <td>1.401459</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>-0.998806</td>\n",
       "      <td>-0.798887</td>\n",
       "      <td>0.550593</td>\n",
       "      <td>-0.330366</td>\n",
       "      <td>0.516867</td>\n",
       "      <td>-0.137979</td>\n",
       "      <td>Anita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>-0.042547</td>\n",
       "      <td>-0.553033</td>\n",
       "      <td>0.398286</td>\n",
       "      <td>-0.168912</td>\n",
       "      <td>-0.801785</td>\n",
       "      <td>0.209301</td>\n",
       "      <td>0.224039</td>\n",
       "      <td>-0.109003</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>-0.598394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.876224</td>\n",
       "      <td>0.774390</td>\n",
       "      <td>-0.794122</td>\n",
       "      <td>0.191423</td>\n",
       "      <td>-0.405578</td>\n",
       "      <td>0.107491</td>\n",
       "      <td>0.157718</td>\n",
       "      <td>0.710793</td>\n",
       "      <td>1.050327</td>\n",
       "      <td>Anita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>-0.111479</td>\n",
       "      <td>-0.695575</td>\n",
       "      <td>0.106272</td>\n",
       "      <td>-0.927812</td>\n",
       "      <td>0.374847</td>\n",
       "      <td>1.148341</td>\n",
       "      <td>-0.268901</td>\n",
       "      <td>0.298166</td>\n",
       "      <td>0.316038</td>\n",
       "      <td>0.019069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.694429</td>\n",
       "      <td>-0.053003</td>\n",
       "      <td>0.800918</td>\n",
       "      <td>0.053875</td>\n",
       "      <td>-0.224970</td>\n",
       "      <td>0.533747</td>\n",
       "      <td>-0.135428</td>\n",
       "      <td>0.331979</td>\n",
       "      <td>0.782654</td>\n",
       "      <td>Anita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>-0.286212</td>\n",
       "      <td>-0.381350</td>\n",
       "      <td>-0.078657</td>\n",
       "      <td>-0.151145</td>\n",
       "      <td>0.115799</td>\n",
       "      <td>0.271336</td>\n",
       "      <td>-0.105777</td>\n",
       "      <td>0.057974</td>\n",
       "      <td>0.275087</td>\n",
       "      <td>-0.147204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317215</td>\n",
       "      <td>0.014155</td>\n",
       "      <td>-0.480993</td>\n",
       "      <td>-0.011739</td>\n",
       "      <td>0.706245</td>\n",
       "      <td>-0.050755</td>\n",
       "      <td>0.089455</td>\n",
       "      <td>0.571589</td>\n",
       "      <td>0.172153</td>\n",
       "      <td>Anita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>-0.532125</td>\n",
       "      <td>-0.897782</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>-0.430264</td>\n",
       "      <td>-0.186873</td>\n",
       "      <td>0.834145</td>\n",
       "      <td>-0.142402</td>\n",
       "      <td>-0.302994</td>\n",
       "      <td>0.495656</td>\n",
       "      <td>-0.011855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.611472</td>\n",
       "      <td>0.950395</td>\n",
       "      <td>0.440137</td>\n",
       "      <td>-0.634771</td>\n",
       "      <td>-0.564941</td>\n",
       "      <td>0.389278</td>\n",
       "      <td>0.165742</td>\n",
       "      <td>-0.040008</td>\n",
       "      <td>0.847228</td>\n",
       "      <td>Anita</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "345 -0.044982  0.141337  0.669841 -1.055132 -0.554959 -0.094389  0.808895   \n",
       "346 -0.042547 -0.553033  0.398286 -0.168912 -0.801785  0.209301  0.224039   \n",
       "347 -0.111479 -0.695575  0.106272 -0.927812  0.374847  1.148341 -0.268901   \n",
       "348 -0.286212 -0.381350 -0.078657 -0.151145  0.115799  0.271336 -0.105777   \n",
       "349 -0.532125 -0.897782  0.337300 -0.430264 -0.186873  0.834145 -0.142402   \n",
       "\n",
       "            7         8         9  ...       119       120       121  \\\n",
       "345  0.614182  0.682296  0.018864  ... -0.020778  1.401459  0.001832   \n",
       "346 -0.109003  0.097656 -0.598394  ... -0.876224  0.774390 -0.794122   \n",
       "347  0.298166  0.316038  0.019069  ... -0.694429 -0.053003  0.800918   \n",
       "348  0.057974  0.275087 -0.147204  ...  0.317215  0.014155 -0.480993   \n",
       "349 -0.302994  0.495656 -0.011855  ... -0.611472  0.950395  0.440137   \n",
       "\n",
       "          122       123       124       125       126       127  target  \n",
       "345 -0.998806 -0.798887  0.550593 -0.330366  0.516867 -0.137979   Anita  \n",
       "346  0.191423 -0.405578  0.107491  0.157718  0.710793  1.050327   Anita  \n",
       "347  0.053875 -0.224970  0.533747 -0.135428  0.331979  0.782654   Anita  \n",
       "348 -0.011739  0.706245 -0.050755  0.089455  0.571589  0.172153   Anita  \n",
       "349 -0.634771 -0.564941  0.389278  0.165742 -0.040008  0.847228   Anita  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  np.array(df.drop(\"target\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.1033583 , -1.2409006 ,  0.2770695 ,  0.28035042, -0.74168307,\n",
       "       -0.68314165,  0.4462086 , -3.0068402 , -0.5803412 ,  0.43669233,\n",
       "       -2.0454152 ,  0.48715952,  1.2720518 ,  0.3470364 ,  0.49793446,\n",
       "        0.9785635 , -1.3363703 , -0.42026234, -0.21643656,  0.1576004 ,\n",
       "       -0.24126655,  0.8240571 ,  0.68876505,  0.44611543,  0.7171621 ,\n",
       "       -0.14826632,  1.0665091 , -0.49170065,  1.0849091 ,  1.0539716 ,\n",
       "        0.7888707 , -1.3220723 , -0.01053585,  1.4388478 ,  0.48437455,\n",
       "        0.16777575,  0.48983663, -0.7492657 ,  1.1551208 ,  0.42781287,\n",
       "        1.1434963 , -0.38328943, -0.8932575 , -0.5075193 ,  1.6439264 ,\n",
       "       -0.2748093 , -1.5193338 ,  1.8231103 , -1.0131761 ,  0.15710998,\n",
       "       -1.7632643 ,  1.910747  , -0.49773106,  1.6818883 , -1.2201595 ,\n",
       "        0.6375461 ,  0.7480596 ,  0.48711985, -0.4464571 , -0.56985873,\n",
       "        1.4008503 , -0.8984292 , -1.0154054 , -0.15562105, -0.9848897 ,\n",
       "        1.5537295 ,  2.1056643 , -1.1798558 , -2.1115782 , -0.71015143,\n",
       "        1.024355  ,  0.56054187,  1.963278  ,  0.29047036, -0.12359919,\n",
       "       -2.2468464 ,  0.46798775, -0.77847284, -0.0199277 , -0.454804  ,\n",
       "       -1.5479618 ,  1.3225961 ,  0.8966304 ,  2.0198245 , -0.09001137,\n",
       "       -0.4439266 ,  0.02486661, -0.29917058,  1.0697894 , -0.23250419,\n",
       "       -1.5873384 , -0.07824114,  1.2583752 ,  1.0146511 ,  1.0454161 ,\n",
       "        0.4243989 ,  0.4997876 ,  0.86670715, -0.38088092, -0.3276422 ,\n",
       "        1.6610364 ,  0.24149865, -0.8420771 ,  1.318064  ,  0.01395509,\n",
       "        1.5439013 , -0.68036264,  0.5647346 ,  0.6067941 , -0.9656259 ,\n",
       "        0.04424966,  0.59184754,  2.3829892 , -1.1451253 ,  0.8649209 ,\n",
       "        0.31581736,  0.27845472,  0.4691571 , -1.8084586 , -0.7688653 ,\n",
       "        1.4233942 ,  0.37253585,  1.0491067 , -2.1072721 , -0.15993167,\n",
       "        1.899448  ,  1.4102762 , -0.5102992 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Anita', 'Anita', 'Anita',\n",
       "       'Anita', 'Anita', 'Anita', 'Anita', 'Anita', 'Anita', 'Anita',\n",
       "       'Anita', 'Anita', 'Anita', 'Anita', 'Anita', 'Anita', 'Anita',\n",
       "       'Anita', 'Anita', 'Anita', 'Anita', 'Anita', 'Anita', 'Anita',\n",
       "       'Anita', 'Anita', 'Anita', 'Anita', 'Anita', 'Anita', 'Anita',\n",
       "       'Anita', 'Anita', 'Anita'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misturando Tudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = shuffle(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Anita', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Anita',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Anita', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Anita', 'Anita', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Anita', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Anita', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Anita', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Anita', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Anita', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Anita', 'Anita', 'Amanda', 'Amanda', 'Amanda', 'Anita',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Anita', 'Anita',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Anita', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Anita', 'Amanda', 'Anita', 'Anita',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Anita',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Anita',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Anita', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Anita', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Anita', 'Amanda', 'Anita', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Anita', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Anita',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Anita',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Anita', 'Amanda', 'Anita', 'Amanda',\n",
       "       'Amanda', 'Anita', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Anita',\n",
       "       'Amanda', 'Anita', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda', 'Amanda', 'Anita', 'Amanda', 'Amanda',\n",
       "       'Amanda', 'Amanda'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratar Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_encoder.fit(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = out_encoder.transform(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(\"faces_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>-0.515415</td>\n",
       "      <td>-0.364444</td>\n",
       "      <td>0.376679</td>\n",
       "      <td>-1.169360</td>\n",
       "      <td>-0.171952</td>\n",
       "      <td>-0.082919</td>\n",
       "      <td>0.334564</td>\n",
       "      <td>1.287003</td>\n",
       "      <td>0.722706</td>\n",
       "      <td>-0.145439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337408</td>\n",
       "      <td>0.911319</td>\n",
       "      <td>0.387249</td>\n",
       "      <td>-0.663056</td>\n",
       "      <td>-0.548511</td>\n",
       "      <td>0.864339</td>\n",
       "      <td>-0.452718</td>\n",
       "      <td>0.657906</td>\n",
       "      <td>0.308859</td>\n",
       "      <td>Anita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>-0.053085</td>\n",
       "      <td>-0.128445</td>\n",
       "      <td>-0.045827</td>\n",
       "      <td>-0.832958</td>\n",
       "      <td>-0.650303</td>\n",
       "      <td>0.017163</td>\n",
       "      <td>0.393169</td>\n",
       "      <td>1.088605</td>\n",
       "      <td>0.715595</td>\n",
       "      <td>-0.157572</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159320</td>\n",
       "      <td>1.508870</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>-0.853419</td>\n",
       "      <td>-0.660579</td>\n",
       "      <td>0.600529</td>\n",
       "      <td>-0.557777</td>\n",
       "      <td>0.683590</td>\n",
       "      <td>0.084083</td>\n",
       "      <td>Anita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.088084</td>\n",
       "      <td>0.431689</td>\n",
       "      <td>0.516850</td>\n",
       "      <td>-0.994467</td>\n",
       "      <td>-0.214396</td>\n",
       "      <td>0.053245</td>\n",
       "      <td>0.753357</td>\n",
       "      <td>0.683375</td>\n",
       "      <td>0.752360</td>\n",
       "      <td>0.049482</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217035</td>\n",
       "      <td>1.238264</td>\n",
       "      <td>-0.065421</td>\n",
       "      <td>-0.967712</td>\n",
       "      <td>-0.686561</td>\n",
       "      <td>0.367021</td>\n",
       "      <td>-0.701336</td>\n",
       "      <td>0.354293</td>\n",
       "      <td>0.018954</td>\n",
       "      <td>Anita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.226625</td>\n",
       "      <td>-0.122337</td>\n",
       "      <td>-0.277066</td>\n",
       "      <td>-0.220689</td>\n",
       "      <td>-0.309456</td>\n",
       "      <td>0.359915</td>\n",
       "      <td>0.060279</td>\n",
       "      <td>-0.154357</td>\n",
       "      <td>-0.121863</td>\n",
       "      <td>-0.219565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.322420</td>\n",
       "      <td>-0.605390</td>\n",
       "      <td>-0.115777</td>\n",
       "      <td>0.365695</td>\n",
       "      <td>-0.089305</td>\n",
       "      <td>-0.025769</td>\n",
       "      <td>0.487779</td>\n",
       "      <td>0.257137</td>\n",
       "      <td>Anita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-0.875248</td>\n",
       "      <td>-0.335100</td>\n",
       "      <td>0.113108</td>\n",
       "      <td>-0.506653</td>\n",
       "      <td>-0.538222</td>\n",
       "      <td>0.073820</td>\n",
       "      <td>0.108364</td>\n",
       "      <td>0.243283</td>\n",
       "      <td>0.086968</td>\n",
       "      <td>-0.480519</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.538318</td>\n",
       "      <td>0.667571</td>\n",
       "      <td>-0.554510</td>\n",
       "      <td>0.297783</td>\n",
       "      <td>0.589745</td>\n",
       "      <td>0.358634</td>\n",
       "      <td>-0.324054</td>\n",
       "      <td>1.172567</td>\n",
       "      <td>0.563215</td>\n",
       "      <td>Anita</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "135 -0.515415 -0.364444  0.376679 -1.169360 -0.171952 -0.082919  0.334564   \n",
       "136 -0.053085 -0.128445 -0.045827 -0.832958 -0.650303  0.017163  0.393169   \n",
       "137  0.088084  0.431689  0.516850 -0.994467 -0.214396  0.053245  0.753357   \n",
       "138  0.226625 -0.122337 -0.277066 -0.220689 -0.309456  0.359915  0.060279   \n",
       "139 -0.875248 -0.335100  0.113108 -0.506653 -0.538222  0.073820  0.108364   \n",
       "\n",
       "            7         8         9  ...       119       120       121  \\\n",
       "135  1.287003  0.722706 -0.145439  ... -0.337408  0.911319  0.387249   \n",
       "136  1.088605  0.715595 -0.157572  ... -0.159320  1.508870  0.259740   \n",
       "137  0.683375  0.752360  0.049482  ... -0.217035  1.238264 -0.065421   \n",
       "138 -0.154357 -0.121863 -0.219565  ...  0.005224  0.322420 -0.605390   \n",
       "139  0.243283  0.086968 -0.480519  ... -0.538318  0.667571 -0.554510   \n",
       "\n",
       "          122       123       124       125       126       127  target  \n",
       "135 -0.663056 -0.548511  0.864339 -0.452718  0.657906  0.308859   Anita  \n",
       "136 -0.853419 -0.660579  0.600529 -0.557777  0.683590  0.084083   Anita  \n",
       "137 -0.967712 -0.686561  0.367021 -0.701336  0.354293  0.018954   Anita  \n",
       "138 -0.115777  0.365695 -0.089305 -0.025769  0.487779  0.257137   Anita  \n",
       "139  0.297783  0.589745  0.358634 -0.324054  1.172567  0.563215   Anita  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "valX = np.array(df_val.drop(\"target\", axis=1))\n",
    "valY = np.array(df_val.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_encoder.fit(valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valY = out_encoder.transform(valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliando Algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = knn.predict(trainX)\n",
    "yhat_val = knn.predict(valX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(model_name, valY, yhat_val):\n",
    "    \n",
    "    cm = confusion_matrix(valY, yhat_val)\n",
    "    total = sum(sum(cm))\n",
    "    acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "    sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "    \n",
    "    print(\"Modelo: {}\".format(model_name))\n",
    "    print(\"Acurácia: {:.4f}\".format(acc))\n",
    "    print(\"Sensitividade: {:.4f}\".format(sensitivity))\n",
    "    print(\"Especificidade: {:.4f}\".format(specificity))\n",
    "    \n",
    "    from mlxtend.plotting import plot_confusion_matrix\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=cm , figsize=(5, 5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: KNN\n",
      "Acurácia: 0.9286\n",
      "Sensitividade: 0.9727\n",
      "Especificidade: 0.7667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_confusion_matrix(\"KNN\", valY, yhat_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = svm.predict(trainX)\n",
    "yhat_val = svm.predict(valX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: SVM\n",
      "Acurácia: 0.9643\n",
      "Sensitividade: 1.0000\n",
      "Especificidade: 0.8333\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_confusion_matrix(\"SVM\", valY, yhat_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = to_categorical(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "valY = to_categorical(valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 8,386\n",
      "Trainable params: 8,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation=\"relu\", input_shape=(128,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 350 samples\n",
      "Epoch 1/100\n",
      "350/350 [==============================] - 1s 4ms/sample - loss: 0.2542 - accuracy: 0.9000\n",
      "Epoch 2/100\n",
      "350/350 [==============================] - 0s 764us/sample - loss: 0.1400 - accuracy: 0.9371\n",
      "Epoch 3/100\n",
      "350/350 [==============================] - 0s 482us/sample - loss: 0.0794 - accuracy: 0.9743\n",
      "Epoch 4/100\n",
      "350/350 [==============================] - 0s 502us/sample - loss: 0.0706 - accuracy: 0.9686\n",
      "Epoch 5/100\n",
      "350/350 [==============================] - 0s 507us/sample - loss: 0.0402 - accuracy: 0.9886\n",
      "Epoch 6/100\n",
      "350/350 [==============================] - 0s 496us/sample - loss: 0.0399 - accuracy: 0.9943\n",
      "Epoch 7/100\n",
      "350/350 [==============================] - 0s 530us/sample - loss: 0.0238 - accuracy: 0.9971: 0s - loss: 0.0233 - accuracy: 1.00\n",
      "Epoch 8/100\n",
      "350/350 [==============================] - 0s 718us/sample - loss: 0.0249 - accuracy: 0.9914\n",
      "Epoch 9/100\n",
      "350/350 [==============================] - 0s 496us/sample - loss: 0.0220 - accuracy: 0.9943\n",
      "Epoch 10/100\n",
      "350/350 [==============================] - 0s 510us/sample - loss: 0.0154 - accuracy: 0.9971\n",
      "Epoch 11/100\n",
      "350/350 [==============================] - 0s 510us/sample - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "350/350 [==============================] - 0s 504us/sample - loss: 0.0239 - accuracy: 0.9943\n",
      "Epoch 13/100\n",
      "350/350 [==============================] - 0s 692us/sample - loss: 0.0119 - accuracy: 0.9971\n",
      "Epoch 14/100\n",
      "350/350 [==============================] - 0s 672us/sample - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "350/350 [==============================] - 0s 527us/sample - loss: 0.0121 - accuracy: 0.9971\n",
      "Epoch 16/100\n",
      "350/350 [==============================] - 0s 715us/sample - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "350/350 [==============================] - 0s 521us/sample - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "350/350 [==============================] - 0s 559us/sample - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "350/350 [==============================] - 0s 963us/sample - loss: 0.0072 - accuracy: 0.9971\n",
      "Epoch 20/100\n",
      "350/350 [==============================] - 0s 513us/sample - loss: 0.0078 - accuracy: 0.9971\n",
      "Epoch 21/100\n",
      "350/350 [==============================] - 0s 539us/sample - loss: 0.0108 - accuracy: 0.9971\n",
      "Epoch 22/100\n",
      "350/350 [==============================] - 0s 581us/sample - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "350/350 [==============================] - 0s 533us/sample - loss: 0.0153 - accuracy: 0.9971\n",
      "Epoch 24/100\n",
      "350/350 [==============================] - 0s 1ms/sample - loss: 0.0105 - accuracy: 0.9971\n",
      "Epoch 25/100\n",
      "350/350 [==============================] - 0s 641us/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "350/350 [==============================] - 0s 635us/sample - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "350/350 [==============================] - 0s 690us/sample - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "350/350 [==============================] - 0s 878us/sample - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "350/350 [==============================] - 0s 784us/sample - loss: 0.0169 - accuracy: 0.9943\n",
      "Epoch 30/100\n",
      "350/350 [==============================] - 0s 724us/sample - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "350/350 [==============================] - 0s 732us/sample - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "350/350 [==============================] - 0s 678us/sample - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "350/350 [==============================] - 0s 818us/sample - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "350/350 [==============================] - 0s 718us/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "350/350 [==============================] - 0s 994us/sample - loss: 0.0069 - accuracy: 0.9971\n",
      "Epoch 36/100\n",
      "350/350 [==============================] - 0s 533us/sample - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "350/350 [==============================] - 0s 681us/sample - loss: 7.2907e-04 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "350/350 [==============================] - 0s 593us/sample - loss: 0.0112 - accuracy: 0.9943\n",
      "Epoch 39/100\n",
      "350/350 [==============================] - 0s 541us/sample - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "350/350 [==============================] - 0s 490us/sample - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "350/350 [==============================] - 0s 672us/sample - loss: 0.0045 - accuracy: 0.9971\n",
      "Epoch 42/100\n",
      "350/350 [==============================] - 0s 578us/sample - loss: 0.0036 - accuracy: 0.9971\n",
      "Epoch 43/100\n",
      "350/350 [==============================] - 0s 604us/sample - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "350/350 [==============================] - 0s 672us/sample - loss: 7.1332e-04 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "350/350 [==============================] - 0s 621us/sample - loss: 9.5499e-04 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "350/350 [==============================] - 0s 527us/sample - loss: 5.1357e-04 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "350/350 [==============================] - 0s 647us/sample - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "350/350 [==============================] - 0s 670us/sample - loss: 4.1740e-04 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "350/350 [==============================] - 0s 638us/sample - loss: 9.1827e-04 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "350/350 [==============================] - 0s 672us/sample - loss: 2.4807e-04 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "350/350 [==============================] - 0s 581us/sample - loss: 2.2211e-04 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "350/350 [==============================] - 0s 621us/sample - loss: 8.9635e-04 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "350/350 [==============================] - 0s 818us/sample - loss: 1.5542e-04 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "350/350 [==============================] - 0s 576us/sample - loss: 0.0010 - accuracy: 1.0000: 0s - loss: 0.0018 - accuracy: 1.00\n",
      "Epoch 55/100\n",
      "350/350 [==============================] - 0s 578us/sample - loss: 2.1060e-04 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "350/350 [==============================] - 0s 524us/sample - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "350/350 [==============================] - 0s 559us/sample - loss: 6.5872e-04 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "350/350 [==============================] - 0s 863us/sample - loss: 0.0043 - accuracy: 0.9971\n",
      "Epoch 59/100\n",
      "350/350 [==============================] - 0s 576us/sample - loss: 0.0093 - accuracy: 0.9971\n",
      "Epoch 60/100\n",
      "350/350 [==============================] - 0s 624us/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "350/350 [==============================] - 0s 453us/sample - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "350/350 [==============================] - 0s 519us/sample - loss: 3.4971e-04 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "350/350 [==============================] - 0s 1ms/sample - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "350/350 [==============================] - 0s 530us/sample - loss: 7.0636e-04 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "350/350 [==============================] - 0s 564us/sample - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "350/350 [==============================] - 0s 841us/sample - loss: 0.0075 - accuracy: 0.9943\n",
      "Epoch 67/100\n",
      "350/350 [==============================] - 0s 615us/sample - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "350/350 [==============================] - 0s 769us/sample - loss: 1.0522e-04 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "350/350 [==============================] - 0s 464us/sample - loss: 4.1175e-05 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "350/350 [==============================] - 0s 470us/sample - loss: 8.8096e-04 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "350/350 [==============================] - 0s 567us/sample - loss: 7.4049e-04 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "350/350 [==============================] - 0s 573us/sample - loss: 1.3750e-04 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "350/350 [==============================] - 0s 701us/sample - loss: 7.7944e-05 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "350/350 [==============================] - 0s 715us/sample - loss: 1.5620e-04 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "350/350 [==============================] - 0s 490us/sample - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 576us/sample - loss: 0.0023 - accuracy: 0.9971\n",
      "Epoch 77/100\n",
      "350/350 [==============================] - 0s 578us/sample - loss: 2.8315e-04 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "350/350 [==============================] - 0s 513us/sample - loss: 0.0022 - accuracy: 0.9971\n",
      "Epoch 79/100\n",
      "350/350 [==============================] - 0s 644us/sample - loss: 3.9801e-04 - accuracy: 1.0000 - loss: 2.4410e-04 - accuracy: 1.\n",
      "Epoch 80/100\n",
      "350/350 [==============================] - 0s 618us/sample - loss: 1.4804e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "350/350 [==============================] - 0s 499us/sample - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "350/350 [==============================] - 0s 519us/sample - loss: 1.1487e-04 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "350/350 [==============================] - 0s 801us/sample - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "350/350 [==============================] - 0s 539us/sample - loss: 4.4226e-04 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "350/350 [==============================] - 0s 541us/sample - loss: 3.3799e-05 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "350/350 [==============================] - 0s 638us/sample - loss: 7.3394e-05 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "350/350 [==============================] - 0s 504us/sample - loss: 3.4922e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "350/350 [==============================] - 0s 521us/sample - loss: 1.1411e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "350/350 [==============================] - 0s 547us/sample - loss: 2.2373e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "350/350 [==============================] - 0s 644us/sample - loss: 3.7640e-05 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "350/350 [==============================] - 0s 507us/sample - loss: 4.3279e-05 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "350/350 [==============================] - 0s 724us/sample - loss: 2.4858e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "350/350 [==============================] - 0s 801us/sample - loss: 1.5214e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "350/350 [==============================] - 0s 670us/sample - loss: 5.9179e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "350/350 [==============================] - 0s 769us/sample - loss: 0.0032 - accuracy: 0.9971\n",
      "Epoch 96/100\n",
      "350/350 [==============================] - 0s 678us/sample - loss: 3.6024e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "350/350 [==============================] - 0s 527us/sample - loss: 1.7548e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "350/350 [==============================] - 0s 533us/sample - loss: 0.0075 - accuracy: 0.9943\n",
      "Epoch 99/100\n",
      "350/350 [==============================] - 0s 499us/sample - loss: 0.0080 - accuracy: 0.9943\n",
      "Epoch 100/100\n",
      "350/350 [==============================] - 0s 681us/sample - loss: 0.0068 - accuracy: 0.9971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22c46f0ab08>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=100, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = model.predict(trainX)\n",
    "yhat_val = model.predict(valX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_val = np.argmax(yhat_val, axis=1)\n",
    "yhat_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valY = np.argmax(valY, axis=1)\n",
    "valY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: KERAS\n",
      "Acurácia: 0.9857\n",
      "Sensitividade: 0.9818\n",
      "Especificidade: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_confusion_matrix(\"KERAS\", valY, yhat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('faces.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
